# Retrospective: Plugin Debug → mz_forwarder Infrastructure Deep Dive

**Date**: 2025-12-28 17:10-20:28
**Duration**: ~3 hours
**Focus**: Plugin debugging, project-incubate.sh, mz_forwarder infrastructure exploration

---

## What We Did

### 1. Plugin System Debugging (17:10-17:44)

User reported `/plugin` showing "(no content)" in fitbit-analysis project.

**Investigation**:
- Traced `~/.claude/plugins/installed_plugins.json`
- Found stale entries pointing to non-existent version paths
- Registry said `1.0.0`, cache only had `1.5.0+`
- Duplicate entries: `@nat-plugins` AND `@nat-agents-core`

**Root cause**: Version path mismatch → silent failure

**Fix**: Cleared registry, fresh install worked.

### 2. Created project-incubate.sh (17:44-18:10)

Script handles:
- URL input → ghq clone if needed → symlink
- Path input → direct symlink
- Auto-register slug in slugs.yaml
- Idempotent checks

Updated `/project` command with Scripts section.

### 3. mz_forwarder Infrastructure Exploration (20:04-20:28)

User asked to trace `00_mz_forwarder` project:

**Discovered architecture**:
```
IoT Devices → MQTT (1883) → 21x Telegraf → Multiple Outputs
                                ↓
                    ┌───────────┼───────────┐
                    ↓           ↓           ↓
              InfluxDB(CMU)  Workers    InfluxDB(DO)❌
```

**Server: mqtt.laris.co** (103.253.72.197)
- 50+ Docker containers
- 21 Telegraf instances (per device model)
- Mosquitto MQTT broker
- Flask API, Grafana, Prometheus, EMQX
- Actix HTTP→MQTT bridge

**Found issues**:
- InfluxDB at 128.199.205.93:8087 is DOWN (100% packet loss)
- Services still functional via other outputs

### 4. Discovered Actix HTTP→MQTT Bridge

Found `dustboy-api-actix` in `laris-co/06_api-nodejs` repo:
- Rust/Actix-web server on port 8080
- HTTP endpoints for device models (t1, t2, t3, t4, arkad, iv)
- Bridges HTTP POST → MQTT publish
- Not cloned locally (only on server via GitHub Actions)

---

## What Worked

1. **SSH access for live debugging** - Could inspect running containers, check logs, verify connectivity
2. **Script approach** - One bash call instead of multiple tool calls
3. **Parallel trace agents** - 5 agents searching simultaneously for project context
4. **gh CLI** - Fast repo discovery across orgs

---

## What Didn't Work

1. **Plugin system fails silently** - No errors when registry→cache path mismatch
2. **InfluxDB output DOWN** - 128.199.205.93 unreachable, needs investigation
3. **Repo naming** - `06_api-nodejs` contains Rust code (misleading)

---

## AI Diary

This session evolved unexpectedly. Started with a plugin debugging task - fairly contained. Fixed that with the nuclear option (clear registry, reinstall). Then created the incubate script to complete yesterday's work.

But then the user wanted to trace `mz_forwarder`, and suddenly I was SSH-ing into a production MQTT server, inspecting 50+ Docker containers, tracing data flows from IoT sensors through Telegraf to InfluxDB.

What struck me was the complexity hidden behind a simple symlink. `00_mz_forwarder` isn't just a repo - it's the orchestration layer for an entire IoT data pipeline. FloodBoy water sensors, DustBoy air quality monitors, Model-N/T variants - all feeding through this system to multiple databases and Cloudflare Workers.

Finding the dead InfluxDB (128.199.205.93) was concerning - 100% packet loss. But the system is resilient - data still flows to the CMU InfluxDB and Cloudflare Workers. Graceful degradation in action.

The Actix discovery was interesting. A Rust HTTP→MQTT bridge hidden inside a repo called "06_api-nodejs". Classic legacy naming. The code itself is clean - Actix-web 4, proper connection pooling, device model validation.

Three hours felt like a journey through infrastructure archaeology.

---

## Honest Feedback

**What went well**:
- User gave clear directions and corrected course when needed
- SSH access enabled real-time debugging
- Built complete mental model of mz_forwarder architecture

**Friction points**:
1. Plugin system needs better error messages
2. Dead InfluxDB should have alerting
3. Repo naming conventions inconsistent (06_api-nodejs = Rust)

**What I should have done differently**:
- Could have used the incubate script immediately instead of manual commands
- Should have offered to check InfluxDB status earlier
- Could have created a diagram earlier in the exploration

---

## Key Decisions

| Decision | Rationale |
|----------|-----------|
| Clear plugin registry vs fix paths | Fresh install = known good state |
| SSH into server for debugging | Live inspection > reading old configs |
| Trace with parallel agents | 5x faster context gathering |
| Don't restart services | All healthy, no need to disrupt |

---

## Infrastructure Summary

### mqtt.laris.co (103.253.72.197)

**Ports**:
- 22: SSH
- 80/443: APISIX gateway
- 1883/8883: MQTT
- 3000: Grafana
- 8080: Actix API
- 9090: Prometheus
- 58000: Flask API

**External Dependencies**:
| Service | URL | Status |
|---------|-----|--------|
| InfluxDB (CMU) | influxdb2024.cmuccdc.org | ✅ |
| InfluxDB (DO) | 128.199.205.93:8087 | ❌ DOWN |
| Workers | dustboy-health.laris.workers.dev | ✅ |

**Repos Involved**:
- `laris-co/00_mz_forwarder` - Main orchestration
- `laris-co/06_api-nodejs` - Actix HTTP→MQTT bridge

---

## Files Changed This Session

| File | Change |
|------|--------|
| `scripts/project-incubate.sh` | New - URL/path handling |
| `.claude/commands/project.md` | Added Scripts section |
| `ψ/memory/slugs.yaml` | Added mz-forwarder slugs |
| `~/.claude/plugins/installed_plugins.json` | Cleared stale entries |

---

## Next Steps

- [ ] Investigate dead InfluxDB at 128.199.205.93
- [ ] Incubate `laris-co/06_api-nodejs` for local access
- [ ] Create `scripts/project-learn.sh` (mirror of incubate)
- [ ] Add alerting for InfluxDB output failures
- [ ] Consider renaming repos for clarity

---

*Session: 2025-12-28 17:10-20:28*
