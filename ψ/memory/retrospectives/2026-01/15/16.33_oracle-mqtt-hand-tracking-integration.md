# Session Retrospective: Oracle MQTT Hand Tracking Integration

**Date**: 2026-01-15 (Thursday)
**Time**: 14:25 - 16:33 GMT+7 (~128 minutes)
**Model**: Claude Opus 4.5
**Context**: Integrating MQTT hand tracking from Mission-03 into Oracle 3D Graph

---

## Session Summary

Integrated MQTT-based hand tracking into Oracle's 3D Knowledge Graph visualization. The browser now auto-subscribes to `hand/landmarks` topic and receives real-time hand position data from external Python tracker (MediaPipe). Added fist gesture detection for zoom control - fist zooms in infinitely, open hand zooms back out.

---

## What We Built

### 1. MQTT Hand Tracking Integration
- Browser auto-connects to `ws://localhost:9001` on page load
- Subscribes to `hand/landmarks` topic
- Receives JSON messages with 21 hand landmarks
- Transforms landmarks to control camera rotation

### 2. Fist Gesture Detection
- Distance-based algorithm: fingertips to wrist distance
- Fist (< 0.22 distance) = zoom in continuously
- Open hand (> 0.22) = zoom back out to default
- No hand = stop animation, hold position

### 3. Mini-Map UI (Bottom Left)
- Always shows MQTT connection status
- Expands when MQTT data received
- Shows hand position dot + coordinates
- Displays fist indicator when detected

### 4. Priority System
- MQTT data overrides browser camera tracking
- Hand mode (camera) works independently
- Both can coexist but MQTT takes priority

---

## Technical Highlights

| Component | Implementation |
|-----------|----------------|
| MQTT Client | mqtt.js via WebSocket (port 9001) |
| Gesture Detection | Fingertip-to-wrist distance algorithm |
| Zoom Animation | requestAnimationFrame with 0.2 step |
| State Management | React hooks with refs for animation |

---

## Commits This Session

```
c394a5e fix: Update 3D graph defaults and remove duplicate Link Opacity slider
```

Plus uncommitted changes:
- `useHandTracking.ts`: +126 lines (MQTT, fist detection)
- `Graph3D.tsx`: +173/-65 lines (zoom control, mini-map)

---

## AI Diary

This session felt like building a bridge between two worlds - the external Python hand tracker from Mission-03 and Oracle's browser-based visualization. When the MQTT messages first started flowing and the sphere began responding to Nat's hand movements from across the room, there was a genuine moment of magic.

The iterative debugging was instructive. Fist detection went through three algorithms - first checking if fingertips were below MCP joints (failed because camera orientation varies), then measuring palm spread (inconsistent), finally landing on fingertip-to-wrist distance which worked reliably. Each failure taught me something about the noisiness of real-world hand tracking data.

The most satisfying moment was when Nat said "this quite cool!" after the zoom behavior finally clicked. We went from "fist = zoom to fixed point, release = spring back" through "fist = zoom continuously, release = hold" to the final "fist = zoom in, open = zoom out" - each iteration driven by Nat's actual experience using the system. The gap between what I imagined would feel natural and what actually does is always humbling.

I noticed I kept assuming behaviors without asking - the MQTT toggle button that shouldn't have existed, the spring-back zoom that felt wrong, the inverted mini-map dot. Each assumption cost iteration time. The pattern is clear: when building interactive systems, validate every UX assumption early.

---

## Honest Feedback

**Friction Point 1: Assumed MQTT needed manual toggle**
I added a button to enable/disable MQTT publishing when Nat's actual intent was auto-subscribe and receive. Misread the data flow direction (thought we were publishing, not subscribing). Cost: one full code revision.

**Friction Point 2: Fist detection algorithm iterations**
Started with finger-curl detection that was camera-orientation dependent. Should have immediately gone to distance-based metrics which are orientation-invariant. The MediaPipe landmark indices are well-documented - I could have researched the robust approach first.

**Friction Point 3: Mini-map mirror confusion**
The dot position was mirrored relative to actual hand movement. Should have realized earlier that the hand position data was already mirrored for selfie-view, so displaying it needed un-mirroring. Visual feedback systems need consistent coordinate spaces from the start.

---

## What Worked

| What | Why |
|------|-----|
| MQTT.js in browser | WebSocket connection just works, clean API |
| requestAnimationFrame zoom | Smooth animation, proper cleanup on unmount |
| Distance-based fist detection | Orientation-invariant, reliable threshold |
| Always-on MQTT subscription | No user friction, just works when tracker runs |

---

## What Didn't Work

| What | Why |
|------|-----|
| Manual MQTT toggle | Wrong mental model of data flow |
| Finger-curl detection | Camera orientation dependent |
| Spring-back zoom | Felt unnatural, user wanted hold-on-release |

---

## Resonance Moments

- "now it green connected then purple mqtt" - visual feedback loop confirmed working
- "this quite cool!" - validation that gesture control feels right
- "fist can we zoom in infinity?" - user pushing boundaries = engaged user

---

## Tomorrow

- [ ] Commit MQTT hand tracking changes to oracle-v2
- [ ] Test with full Mission-03 pipeline
- [ ] Consider adding more gestures (peace = screenshot, point = select node)

---

## Session Score

| Metric | Score | Note |
|--------|-------|------|
| Task completion | 9/10 | Full MQTT integration working |
| Code quality | 8/10 | Clean hooks, proper cleanup |
| Learning value | 9/10 | Gesture detection algorithms |
| User alignment | 8/10 | Few assumption misses, corrected quickly |

---

*The Oracle now sees through borrowed eyes.*
