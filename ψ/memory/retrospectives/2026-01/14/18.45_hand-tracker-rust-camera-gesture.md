# Session Retrospective: Hand Tracker Rust - Camera + Gesture

**Date**: 2026-01-14
**Time**: 18:32 - 18:45 (GMT+7)
**Duration**: ~13 minutes
**Location**: BM's Home (Bang Phli) ‚Üí In Car (voice commands)
**Context**: 70%

---

## Executive Summary

Rapid iteration on the Rust hand-tracker-mqtt app. Added camera capture (nokhwa), horizontal mirroring for selfie view, and gesture detection (fist/open hand for zoom in/out). User tested via voice commands while in car.

---

## What We Built

### Camera Integration
- Added `nokhwa` camera capture at 640x480 for speed
- Horizontal mirror for natural selfie view
- Real-time display in minifb window

### Gesture Detection
| Gesture | Color | Spread | Action |
|---------|-------|--------|--------|
| ‡∏Å‡∏≥‡∏°‡∏∑‡∏≠ (fist) | üî¥ Red | < 0.3 | Zoom In |
| ‡πÅ‡∏ö‡∏°‡∏∑‡∏≠ (open) | üîµ Cyan | > 0.5 | Zoom Out |
| Neutral | üü¢ Green | 0.3-0.5 | None |

### MQTT Message Format
```json
{
  "x": 0.5,
  "y": 0.5,
  "confidence": 0.8,
  "gesture": 1,
  "spread": 0.3,
  "timestamp": 123456
}
```

---

## Timeline

| Time | Action |
|------|--------|
| 18:32 | /trace rust app |
| 18:38 | First camera run (1920x1080, slow) |
| 18:40 | Reduced to 640x480, added mirror |
| 18:43 | Added gesture detection |
| 18:45 | Complete, user confirmed working |

---

## Technical Details

### Dependencies Used
- `nokhwa` - Camera capture
- `minifb` - Debug window
- `rumqttc` - MQTT publishing
- `serde_json` - JSON serialization

### Skin Detection Algorithm
```rust
// Simple skin color detection
if r > 100 && g > 50 && b > 30 && r > g && r > b && (r - b) > 15 {
    // Likely skin pixel
}
```

### Spread Calculation
- Collect all skin-colored pixels
- Calculate centroid (average position)
- Measure average distance from centroid
- Normalize to 0-1 range

---

## What Went Well

1. **Voice-driven development**: User gave commands from car, I implemented
2. **Rapid iteration**: 4 builds in 13 minutes
3. **Camera just worked**: nokhwa + macOS camera permission smooth
4. **Simple gesture detection**: Basic but functional

---

## What Could Improve

1. **Better hand model**: Current skin detection is crude, needs ONNX/MediaPipe
2. **Gesture stability**: Needs smoothing/debouncing
3. **Background subtraction**: Would help isolate hand from scene

---

## AI Diary

This was an unusual session - the user was in a car using voice input, and I was implementing a camera app on their MacBook at BM's home. The physical separation added a layer of complexity: I couldn't see what they saw, only hear their feedback.

The "mirror fix" request was classic - of course a selfie camera should be mirrored! I should have done that from the start. The gesture detection was fun to implement - using spread (how dispersed the skin pixels are) as a proxy for open/closed hand is clever but crude. It works in controlled lighting but would fail in many conditions.

What delighted me: when the user said "ok i think this done!" after only 13 minutes of work. We went from "app has no camera" to "gesture detection working" in a very short session. Voice-driven development actually worked!

---

## Honest Feedback

**What DIDN'T work?**

The framerate was initially too slow at 1920x1080. Should have started with 640x480. Also, running apps in background from Claude Code is tricky - they keep closing when the shell resets.

**What was FRUSTRATING?**

1. Not being able to see the user's screen
2. The shell cwd resetting after cargo commands
3. macOS camera flash surprised the user

**What DELIGHTED me?**

Voice-driven development from a car! The user spoke naturally in Thai, I parsed intent, implemented code, rebuilt, and they tested remotely. This is the Oracle vision in action.

---

## Files Changed

| Repo | File | Change |
|------|------|--------|
| hand-tracker-mqtt | src/main.rs | Camera + gesture detection |
| hand-tracker-mqtt | Cargo.toml | Dependencies |
| hand-tracker-mqtt | README.md | Documentation |

---

## Commits

- `2fadcda` (hand-tracker-mqtt): feat: Camera capture + gesture detection

---

## Next Steps

- [ ] Add ONNX hand landmark model
- [ ] Connect to Three.js Graph3D via MQTT subscriber
- [ ] Add gesture smoothing/debouncing
- [ ] Push hand-tracker-mqtt to GitHub

---

*Session: hand-tracker-rust-camera-gesture*
*Voice-driven development from car to home Mac*
