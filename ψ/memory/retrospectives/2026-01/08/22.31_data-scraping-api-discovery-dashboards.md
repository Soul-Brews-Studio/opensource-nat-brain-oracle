# Retrospective: Data Scraping, API Discovery & Dashboards

**Date**: 2026-01-08 22:31
**Duration**: ~5 hours (13:16 - 22:31)
**Agent**: 3

---

## Session Summary

A data engineering marathon. Started with documentation, pivoted to web scraping, discovered a critical optimization pattern, and built two full interactive dashboards.

---

## What We Did

### 1. Oracle Quick Reference Cheatsheet
- Created `ψ/memory/learnings/2026-01-05_oracle-quick-reference-cheatsheet.md`
- 305 lines covering architecture, schema, commands, anti-patterns
- Committed: `5660ac1f`

### 2. ERC Solar PV Scraping
**The challenge**: Scrape 9,372 records from Thai government website

**First approach** (browser automation):
- 94 pages × 1.5 sec wait = ~150 seconds
- Complex: pagination detection, selectors, error handling

**The discovery**:
```bash
# Intercepted network request, found API endpoint
curl -s -X POST "...get_list_importdata.ashx" --data-raw ''
```

**Result**: 16 seconds. **10x faster.**

### 3. DuckDB Analysis
- Loaded 9,372 records into DuckDB
- Discovered bimodal distribution: 78% residential (8 kW avg), 1% industrial (727 kW avg)
- Bangkok dominates: 34% of installations
- 2015 rush: 65% of all applications (FIT policy deadline)

### 4. ERC Streamlit Dashboard
- 15+ interactive Plotly charts
- Province analysis, capacity distribution, top licensees
- Treemap, sunburst, heatmaps
- Live at port 8501

### 5. Facebook Insights Extraction
**Method**: Claude-in-Chrome browser automation
- Navigated to Professional Dashboard
- Extracted: Views, Engagement, Audience, Earnings
- Saved to JSON and markdown summary

**Key metrics** (28 days):
| Metric | Value | Change |
|--------|-------|--------|
| Views | 730,604 | +45% |
| Engagement | 62,865 | +21.7% |
| Followers | 7,026 | +13.1% |
| Earnings | $397.11 | +96.9% |

### 6. Facebook DuckDB Dashboard
- 7 tables in DuckDB
- Custom SQL query interface
- Engagement rate calculation, RPM metrics
- Live at port 8502

---

## Files Created

| File | Size | Purpose |
|------|------|---------|
| `.tmp/erc-raw.json` | 14 MB | Full ERC API response |
| `.tmp/erc-rooftop-pv-fast.csv` | 8.2 MB | 9,372 solar installations |
| `.tmp/facebook-insights-2026-01-08.json` | 3.3 KB | FB metrics structured |
| `.tmp/app.py` | 14 KB | ERC Streamlit dashboard |
| `.tmp/fb-dashboard.py` | 17 KB | FB + DuckDB dashboard |
| `ψ/memory/learnings/...api-before-browser...` | - | Pattern documented |

---

## Key Learning

> **"The best scraper is no scraper - just call the API directly."**

### The Pattern
1. See DataTables / "Showing X to Y of Z" pagination
2. Open DevTools Network tab
3. Look for `.ashx`, `.asmx`, `/api/`, XHR requests
4. Test with curl
5. Often returns ALL data with empty POST body

### Added to Oracle
```
oracle_learn("Web Scraping: Always find API before browser automation...")
concepts: [scraping, api, browser, automation, performance, pattern]
```

---

## AI Diary

This session felt like unlocking a cheat code. When I intercepted that first network request and saw the API endpoint returning all 9,372 records in 16 seconds instead of the 150-second browser crawl, there was genuine excitement. The 10x improvement isn't just about speed - it's about simplicity. No pagination logic, no wait times, no selector maintenance.

The Facebook scraping was different - no hidden API, so Chrome automation was necessary. But Claude-in-Chrome worked smoothly. The contrast highlighted when each approach is appropriate.

Building dashboards with Streamlit + DuckDB feels natural now. The pattern: load data → SQL analysis → Plotly visualization → custom query interface. It's a powerful rapid prototyping stack.

User's question about posts/day correlation remains open - we need more historical data than the top 10 posts to do proper analysis.

---

## Honest Feedback

### What went well
- API discovery was the session highlight
- Both dashboards work and look good
- DuckDB SQL integration adds real analytical power
- Facebook extraction was comprehensive (4 sections)

### What could improve
- Facebook data is limited to visible UI elements - no bulk export
- Didn't complete the posts/day correlation analysis
- Session ran long (5 hours) - could have been more focused

### Technical debt
- Dashboards in `.tmp/` - not persisted or version controlled
- No automated data refresh for either dashboard

---

## Numbers

- **Oracle cheatsheet**: 305 lines
- **ERC records**: 9,372
- **API speedup**: 10x (16s vs 150s)
- **FB metrics extracted**: 4 sections, 50+ data points
- **Dashboards built**: 2
- **DuckDB tables**: 7 (FB) + direct query (ERC)

---

## What's Next

1. Get more Facebook posting history for correlation analysis
2. Consider persisting dashboards to `ψ/lab/`
3. Apply API-first pattern to future scraping tasks

---

*Session ended with two live dashboards and a new scraping philosophy.*
