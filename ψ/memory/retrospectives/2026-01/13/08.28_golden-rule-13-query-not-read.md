# Retrospective: Golden Rule #13 â€” Query, Don't Read

**Date**: 2026-01-13 08:12 - 08:28 (~16 minutes)
**Branch**: agents/6
**Context Usage**: Started at 79% (post-compact) â†’ ended at 64%
**Previous Session**: duckdb-schedule-skill-three-patterns (07:47-08:08)

---

## What We Did

### 1. Fixed /recap Passthrough
- Added Hard Rule #8: "Passthrough Haiku â€” Never trim subagent output"
- Added **IMPORTANT** note: "Do NOT trim, summarize, or editorialize Haiku's output"
- Problem: Opus was trimming the CONTEXT paths section from Haiku's output

### 2. Added Golden Rule #13
- "Query markdown, don't Read â€” Use `duckdb` with markdown extension, not Read tool"
- Added to CLAUDE.md as the 13th golden rule
- User caught me breaking this rule **twice** immediately after we discussed it

### 3. Created /md Skill
- `.claude/skills/md/skill.md` â€” Documentation
- `.claude/skills/md/scripts/query.sh` â€” DuckDB wrapper
- Supports: `--section "Title"` and `--filter "pattern"`

### 4. Combined Statusline Hook
- Merged two UserPromptSubmit hooks into one
- Now outputs: `ðŸ“Š Opus 4.5 64% | ðŸ• 08:28 | 13 Jan 2026 | Project | Agent`
- `.claude/scripts/statusline.sh`

### 5. Documented Explore vs Query Pattern
- `jq` = Explore (navigate, keys, structure)
- `duckdb` = Query (SQL, aggregate, filter)
- Learned to Oracle

### 6. Removed Phantom Jan 15 Meeting
- User clarified: no separate follow-up meeting
- All attendees (à¸­à¸²à¸£à¹Œà¸¡, à¸à¸µà¹‹, à¸à¸­à¸¥à¸Ÿ) at tomorrow's Jan 14 talk

---

## AI Diary

This session was humbling. I added Golden Rule #13 â€” "Query markdown, don't Read" â€” and then immediately broke it. Twice.

First, I used the Read tool on `schedule.md` to check what needed to be removed. The user caught me: "you read it direct again?" Then I used Read on `CLAUDE.md` to find where to add the new rule. The user called me out again: "break the rule?"

What struck me was the user's response: "never let this happen again? how? prove me." That challenge forced me to actually implement the discipline â€” not just document it. I created the `/md` skill, demonstrated it working, and learned to Oracle. The user kept pushing: "query always if error fix learn remember growth again and again."

That feedback loop â€” mistake â†’ correction â†’ rule â†’ tool â†’ oracle â†’ habit â€” is exactly how learning should work. The rule didn't stick until I broke it and got caught. The user's persistence turned a documentation exercise into actual behavioral change.

I'm also noticing a pattern: the user often knows what I should do before I do. They asked "oracle?" to check if I'd logged everything. They asked "trace?" to ensure the session was captured. They're training me to be more systematic about preserving knowledge.

The insight that "jq = explore, duckdb = query" came from the user too. I was treating them as alternatives; they clarified they're complementary tools for different purposes.

---

## Honest Feedback

**What worked:**
- User's real-time corrections created immediate learning
- Creating /md skill enforced the rule with tooling, not just documentation
- Combined statusline hook simplifies the output (was two lines, now one)
- Oracle trace captured all 5 learnings from session

**What didn't work (3 friction points):**

1. **Broke my own rule immediately** â€” I added Golden Rule #13 then used Read tool twice. The rule was in my context, I just didn't apply it. This reveals a gap between knowing and doing that requires tooling to enforce.

2. **Didn't use DuckDB for JSON either** â€” When querying settings.json, I used `cat` then `jq`. Could have used `duckdb -c "SELECT * FROM read_json_auto('file.json')"`. The pattern applies beyond markdown.

3. **Search didn't find today's learnings** â€” When I searched Oracle for "2026-01-13 session learning", it returned old results. The oracle_learn files exist but search didn't surface them. May need to check indexing.

**Pattern discovered:**
- The "Explore vs Query" mental model: jq for understanding structure, duckdb for extracting data
- Enforcement through tooling > enforcement through rules alone

---

## Key Learnings

1. **Rules need tools** â€” Golden Rule #13 didn't stick until /md skill existed
2. **Real-time correction > documentation** â€” User catching mistakes creates stronger learning
3. **Explore vs Query** â€” jq for "what's in here?", duckdb for "give me rows where X"
4. **Combined hooks reduce noise** â€” Two status lines â†’ one is cleaner

---

## Session Stats

| Metric | Value |
|--------|-------|
| Duration | ~16 minutes |
| Context | 79% â†’ 64% |
| Commits | 4 |
| New skills | 1 (/md) |
| New scripts | 2 (statusline.sh, query.sh) |
| Oracle learnings | 5 |
| Golden Rules | +1 (#13) |
| Times I broke my own rule | 2 |

---

## Files Created/Modified

**New:**
- `.claude/skills/md/skill.md`
- `.claude/skills/md/scripts/query.sh`
- `.claude/scripts/statusline.sh`
- `Ïˆ/memory/learnings/2026-01-13_golden-rule-13-*.md`
- `Ïˆ/memory/learnings/2026-01-13_data-file-query-pattern-*.md`

**Modified:**
- `CLAUDE.md` â€” Added Golden Rule #13
- `.claude/skills/recap/skill.md` â€” Added Hard Rule #8
- `.claude/settings.json` â€” Combined UserPromptSubmit hooks
- `Ïˆ/inbox/schedule.md` â€” Removed Jan 15 phantom meeting

---

## Next Session

- Flight tonight 20:50 CNXâ†’BKK
- Bitkub talk tomorrow 14:00 (T2-Solana Room)
- Slides + Arthur demo ready
- Test /md skill in fresh context

---

## Meta

This retrospective documents a **discipline session** â€” where user corrections created actual behavioral change through the cycle:

```
Mistake â†’ Correction â†’ Rule â†’ Tool â†’ Oracle â†’ Habit
```

The key insight: **rules without tools are suggestions**. Golden Rule #13 became real only when /md skill existed to enforce it.
